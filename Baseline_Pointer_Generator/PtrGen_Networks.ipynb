{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/rutts07/.cache/huggingface/datasets/cnn_dailymail/1.0.0/1.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04399585723876953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f9a542bee64e899207269f3a066ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Import the datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", '1.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from rouge import Rouge\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and preprocess the train dataset\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub('\"','', s)\n",
    "    s = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in s.split(\" \")])\n",
    "    s = re.sub(r\"'s\\b\",\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def LoadArticlesAndSummaries(dataset, type, count=0, max_article_length=0, max_summary_length=0):\n",
    "    pairs = []\n",
    "    \n",
    "    if count > len(dataset[type]) or count == 0:\n",
    "        count = len(dataset[type])\n",
    "    \n",
    "    # Choose articles and summaries with length less than max_article_length and max_summary_length\n",
    "    i = 0\n",
    "    num_sents = 0\n",
    "    \n",
    "    if (max_article_length == 0 or max_summary_length == 0):\n",
    "        for i in range(count):\n",
    "            article = normalizeString(dataset[type][i]['article'])\n",
    "            summary = normalizeString(dataset[type][i]['highlights'])\n",
    "            pairs.append([article, summary])\n",
    "            \n",
    "        return pairs\n",
    "            \n",
    "    for i in range(len(dataset[type])):\n",
    "        if (num_sents >= count):\n",
    "            break\n",
    "        \n",
    "        if (len(dataset[type][i]['article'].split()) <= max_article_length and len(dataset[type][i]['highlights'].split()) <= max_summary_length):\n",
    "            pair = []\n",
    "            pair.append(normalizeString(dataset['train'][i]['article']))\n",
    "            pair.append(normalizeString(dataset['train'][i]['highlights']))\n",
    "            pairs.append(pair)\n",
    "            num_sents += 1\n",
    "            # articles.append(normalizeString(dataset['train'][i]['article']))\n",
    "            # summaries.append(normalizeString(dataset['train'][i]['highlights']))\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the vocabulary\n",
    "\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "UNK_token = 3  # Unknown word token\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, pairs):\n",
    "        super(Vocab, self).__init__()\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.num_words = 4  # Count SOS, EOS, PAD\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def wrd2idx(self, word):\n",
    "        if word in self.word2index:\n",
    "            return self.word2index[word]\n",
    "        else:\n",
    "            return UNK_token\n",
    "        \n",
    "    def idx2wrd(self, idx):\n",
    "        if idx in self.index2word:\n",
    "            return self.index2word[idx]\n",
    "        else:\n",
    "            return self.index2word[UNK_token]\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word in self.word2index:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "        else:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.num_words += 1\n",
    "            \n",
    "    def build_vocab(self):        \n",
    "        for pair in self.pairs:\n",
    "            self.add_sentence(pair[0])   # Add only the article to the vocabulary\n",
    "            # self.addSentence(pair[1])\n",
    "            \n",
    "        self.num_words = len(self.word2index)\n",
    "        print(\"Vocabulary created with %d words ...\" % self.num_words)\n",
    "        # return self.num_words\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return self.num_words\n",
    "    \n",
    "    # Modify the vocabulary to include OOV words for copy mechanism\n",
    "    def extend_vocab(self, oovs):\n",
    "        indices = []\n",
    "        for i in range(len(oovs)):\n",
    "            indices.append(self.num_words + i)\n",
    "        oov_dict = dict(zip(indices, oovs))\n",
    "        new_vocab = {**self.index2word, **oov_dict}\n",
    "        # new_vocab = dict(self.index2word.items() + oov_dict.items())           \n",
    "        # print(\"Vocabulary extended with %d words ...\" % len(oovs))\n",
    "        return new_vocab\n",
    "    \n",
    "    def encode_article(self, sentence, oovs=[]):\n",
    "        word2ids = []   \n",
    "        \n",
    "        for word in sentence.split(\" \"):\n",
    "            word2id = self.wrd2idx(word)\n",
    "            if word2id == UNK_token:\n",
    "                if word not in oovs:\n",
    "                    oovs.append(word)\n",
    "                word2ids.append(self.num_words + oovs.index(word))   \n",
    "            else:\n",
    "                word2ids.append(word2id)\n",
    "                \n",
    "        return word2ids, oovs\n",
    "    \n",
    "    def encode_summary(self, sentence, oovs):\n",
    "        word2ids = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word in self.word2index:\n",
    "                word2ids.append(self.word2index[word])\n",
    "                \n",
    "            else:\n",
    "                if word in oovs:\n",
    "                    word2ids.append(self.num_words + oovs.index(word))\n",
    "                else:\n",
    "                    word2ids.append(UNK_token)\n",
    "                    \n",
    "        return word2ids\n",
    "    \n",
    "    def decode_idx2words(self, word2ids, oovs):\n",
    "        words = []\n",
    "        new_vocab = self.extend_vocab(oovs)\n",
    "        \n",
    "        for word2id in word2ids:\n",
    "            if word2id in new_vocab:\n",
    "                words.append(new_vocab[word2id])\n",
    "            else:\n",
    "                words.append(\"UNK\")\n",
    "                \n",
    "        return words\n",
    "    \n",
    "    def trim_vocab(self, min_count=0): \n",
    "        # Re-initialize dictionaries \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.num_words = 4  # Count SOS, EOS, PAD\n",
    "               \n",
    "        for pair in self.pairs:\n",
    "            self.add_sentence(pair[0])\n",
    "            # self.add_sentence(pair[1])\n",
    "\n",
    "        # Remove words below a certain count threshold\n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "               \n",
    "        # Re-initialize dictionaries \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.num_words = 4  # Count SOS, EOS, PAD\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.add_word(word)\n",
    "            \n",
    "        self.num_words = len(keep_words)\n",
    "        # print(\"Vocabulary trimmed to %d words ...\" % self.num_words)\n",
    "        # return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the device\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")    # \"cuda\" if USE_CUDA else \"cpu\"\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define utility functions\n",
    "def trimRareWords(vocab, MIN_COUNT=0):\n",
    "    init_num_words = vocab.vocab_size()\n",
    "    vocab.trim_vocab(MIN_COUNT)  \n",
    "    final_num_words = vocab.vocab_size()\n",
    "    print('Trimmed from {} words to {} words, removing {} words'.format(init_num_words, final_num_words, init_num_words - final_num_words))\n",
    "\n",
    "### Perform length analysis on the dataset\n",
    "def data_analysis(pairs):\n",
    "      print(\"Number of article-summary pairs : %d\" % len(pairs))\n",
    "      \n",
    "      article_word_count = []\n",
    "      summary_word_count = []\n",
    "\n",
    "      # populate the lists with sentence lengths\n",
    "      for i in range(len(pairs)):\n",
    "            article_word_count.append(len(pairs[i][0].split()))\n",
    "            summary_word_count.append(len(pairs[i][1].split()))      \n",
    "\n",
    "      length_df = pd.DataFrame({'text':article_word_count, 'summary': summary_word_count})\n",
    "      length_df.hist(bins = 30)\n",
    "      plt.show()\n",
    "      \n",
    "# Max-Article-length = 400\n",
    "# Max-Summary-length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of article-summary pairs : 30000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs = LoadArticlesAndSummaries(dataset, 'train', 30000, 512, 256)\n",
    "data_analysis(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created with 104571 words ...\n",
      "Trimmed from 104571 words to 29317 words, removing 75254 words\n"
     ]
    }
   ],
   "source": [
    "# Build Vocabulary with the trimmed dataset\n",
    "vocab = Vocab(pairs)\n",
    "vocab.build_vocab()\n",
    "trimRareWords(vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Batch Generator \n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "                \n",
    "    return m\n",
    "\n",
    "def batch_article(article_indexes_batch):\n",
    "    lengths = torch.tensor([len(indexes) for indexes in article_indexes_batch], device='cpu')\n",
    "    padList = zeroPadding(article_indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "def batch_summary(summary_indexes_batch):\n",
    "    max_target_len = max([len(indexes) for indexes in summary_indexes_batch])\n",
    "    padList = zeroPadding(summary_indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask) \n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len \n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    art_ids, sum_ids, oovs = [], [], []\n",
    "    \n",
    "    for pair in pair_batch:\n",
    "        art_id, oovs = voc.encode_article(pair[0], oovs)\n",
    "        sum_id = voc.encode_summary(pair[1], oovs)\n",
    "        \n",
    "        art_ids.append(art_id)\n",
    "        sum_ids.append(sum_id)\n",
    "        \n",
    "    max_oov_len = len(oovs)\n",
    "    \n",
    "    inp_ids, inp_len = batch_article(art_ids)\n",
    "    out_ids, mask, max_out_len = batch_summary(sum_ids)\n",
    "    return inp_ids, inp_len, oovs, out_ids, mask, max_out_len, max_oov_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([470, 5])\n",
      "torch.Size([61, 5])\n",
      "18\n",
      "29317 29335\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# Example for validation\n",
    "small_data = pairs[:100]\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(vocab, [random.choice(small_data) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, oovs, target_variable, mask, max_target_len, max_oov_len = batches\n",
    "new_vocab = vocab.extend_vocab(oovs)\n",
    "\n",
    "# input_variable = input_variable             # B X L\n",
    "# target_variable = target_variable           # B X L\n",
    "\n",
    "# print(\"input_variable:\", input_variable, \"\\n\")\n",
    "# print(\"lengths:\", lengths, \"\\n\")\n",
    "# print(\"target_variable:\", target_variable, \"\\n\")\n",
    "# print(\"mask:\", mask, \"\\n\")\n",
    "# print(\"max_target_len:\", max_target_len)\n",
    "# print(\"oovs:\", oovs)\n",
    "\n",
    "print(input_variable.shape)\n",
    "print(target_variable.shape)\n",
    "print(max_oov_len)\n",
    "print(vocab.num_words, len(new_vocab))\n",
    "print(lengths.shape)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the Encoder\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True, batch_first=True)\n",
    "        self.reduce = nn.Linear(hidden_size*2, hidden_size, bias=True)\n",
    "        \n",
    "    def _init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers*2, batch_size, self.hidden_size).cuda()\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, embedding, hidden=None):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        hidden = self._init_hidden(input_seq.size(0)) if hidden is None else hidden\n",
    "        embedded = embedding(input_seq).cuda()                                          # B X L X H\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        outputs, hidden = self.gru(packed, hidden)                                      # Output = B X L X 2H\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "                \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] \n",
    "        outputs = outputs.cuda()                                                \n",
    "        hidden = torch.relu(self.reduce(torch.cat((hidden[0], hidden[1]), dim=-1))).unsqueeze(0)    # 1 X B X H (Reduce hidden size of GRU)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the Encoder-Decoder Attention\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.V_prime = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.enc_v = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.dec_v = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        \n",
    "    # using multi-layer perceptron attention\n",
    "    def mlp_score(self, decoder_hidden, encoder_output):\n",
    "        enc_feature = self.enc_v(encoder_output).cuda()                        # B X L X H\n",
    "        dec_feature = self.dec_v(decoder_hidden).cuda()                        # B X 1 X H\n",
    "        \n",
    "        scores = enc_feature + dec_feature                              # B X L X H\n",
    "        scores = torch.tanh(scores).cuda()                          # B X L X H\n",
    "        scores = self.V_prime(scores)                                   # B X L X 1\n",
    "        scores = scores.squeeze(-1)                                     # B X L\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, enc_pad_mask=None):\n",
    "        attn_scores = self.mlp_score(decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        # Don't attend over padding\n",
    "        # if enc_pad_mask is not None:\n",
    "            # attn_scores = attn_scores.float().masked_fill_(enc_pad_mask, float('-inf')).type_as(attn_scores)\n",
    "        \n",
    "        attn_dist = F.softmax(attn_scores, dim=-1).unsqueeze(1)  # B X 1 X L\n",
    "        \n",
    "        return attn_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the decoder\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout=0):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout), batch_first=True)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs, embedding, enc_pad_mask=None):\n",
    "        embedded = embedding(input_step)                                # B X H\n",
    "        embedded = self.embedding_dropout(embedded).cuda()          # B X H\n",
    "        \n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)            # B X 1 X H, B X 1 X H\n",
    "        rnn_output = rnn_output.cuda()                       # B X H\n",
    "        attn_dist = self.attn(rnn_output, encoder_outputs, enc_pad_mask)# B X 1 X L\n",
    "        attn_dist = attn_dist.cuda()\n",
    "        \n",
    "        context = torch.bmm(attn_dist, encoder_outputs).cuda()                 # B X 1 X H\n",
    "        \n",
    "        rnn_output = rnn_output.squeeze(1)                             # B X H\n",
    "        context = context.squeeze(1)                                   # B X H\n",
    "        \n",
    "        concat_input = torch.cat((rnn_output, context), 1).cuda()      # B X 2H\n",
    "        concat_output = torch.tanh(self.concat(concat_input)).cuda()   # B X H\n",
    "        \n",
    "        output = self.out(concat_output).cuda()                                # B X V\n",
    "        output = F.softmax(output, dim=-1)                              # B X V\n",
    "        \n",
    "        return output, attn_dist.squeeze(1), context, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pointer-generator network\n",
    "class PtrGen(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout=0):\n",
    "        super(PtrGen, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size      # Vocabulary size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = EncoderRNN(hidden_size, n_layers, dropout).cuda()\n",
    "        self.decoder = AttnDecoderRNN(hidden_size, output_size, n_layers, dropout).cuda()\n",
    "        \n",
    "        # Pointer-generator parameters\n",
    "        self.w_h = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.w_s = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.w_x = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, target_seq, max_target_len, max_oov_len, vocab, enc_pad_mask=None):  \n",
    "        # Vocabulary changes for each batch due to copy mechanism\n",
    "        embedding = nn.Embedding(len(vocab), self.hidden_size)  \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_lengths, embedding)\n",
    "        \n",
    "        # change device\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "        \n",
    "        final_dists = []                                                                # Final dist for NLL loss\n",
    "        # attn_dists = []\n",
    "        for t in range(max_target_len):\n",
    "            decoder_input = target_seq[:, t].unsqueeze(1)                               # B X 1\n",
    "            decoder_input.cuda()\n",
    "            \n",
    "            decoder_outputs = self.decoder(decoder_input, encoder_hidden, encoder_outputs, embedding, enc_pad_mask)\n",
    "            decoder_output, attn_dist, context, decoder_hidden = decoder_outputs\n",
    "            \n",
    "            # change device\n",
    "            decoder_output = decoder_output.cuda()\n",
    "            attn_dist = attn_dist.cuda()\n",
    "            context = context.cuda()\n",
    "            \n",
    "            decoder_hidden = decoder_hidden.squeeze(0)                                  # B X H\n",
    "            decoder_hidden = decoder_hidden.cuda()\n",
    "            \n",
    "            context_feat = self.w_h(context).cuda()                                 # B X 1\n",
    "            decoder_feat = self.w_s(decoder_hidden).cuda()                          # B X 1\n",
    "            input_feat = self.w_x(embedding(decoder_input.squeeze(-1))).cuda()      # B X 1\n",
    "            \n",
    "            p_gen = torch.sigmoid(context_feat + decoder_feat + input_feat).cuda()  # B X 1\n",
    "            vocab_dist = p_gen * decoder_output                                         # B X V\n",
    "            wattn_dist = (1 - p_gen) * attn_dist                                        # B X L\n",
    "            \n",
    "            batch_size = input_seq.size(0)\n",
    "            extra_zeros = torch.zeros(batch_size, max_oov_len).cuda()\n",
    "            \n",
    "            extended_vocab_dist = torch.cat([vocab_dist, extra_zeros], 1).cuda()    # B X V'\n",
    "            final_dist = extended_vocab_dist.scatter_add(1, input_seq, wattn_dist)      # B X V'   \n",
    "            \n",
    "            final_dists.append(final_dist)\n",
    "            # attn_dists.append(attn_dist)\n",
    "            \n",
    "        final_dists = torch.stack(final_dists, dim=-1).cuda()                                  # B X V' X T\n",
    "        # attn_dists = torch.stack(attn_dists, dim=-1)                                  # B X L X T\n",
    "        \n",
    "        return final_dists\n",
    "    \n",
    "    def generate_teacher_forcing(self, input_seq, input_lengths, target_seq, max_target_len, max_oov_len, vocab, enc_pad_mask=None):\n",
    "        # Vocabulary changes for each batch due to copy mechanism\n",
    "        embedding = nn.Embedding(len(vocab), self.hidden_size)  \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_lengths, embedding)\n",
    "        \n",
    "        # change device\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "        \n",
    "        final_dists = []                                                                # Final dist for NLL loss\n",
    "        # attn_dists = []\n",
    "        for t in range(max_target_len):\n",
    "            decoder_input = target_seq[:, t].unsqueeze(1)                               # B X 1\n",
    "            decoder_input.cuda()\n",
    "            \n",
    "            decoder_outputs = self.decoder(decoder_input, encoder_hidden, encoder_outputs, embedding, enc_pad_mask)\n",
    "            decoder_output, attn_dist, context, decoder_hidden = decoder_outputs\n",
    "            \n",
    "            # change device\n",
    "            decoder_output = decoder_output.cuda()\n",
    "            attn_dist = attn_dist.cuda()\n",
    "            context = context.cuda()\n",
    "            \n",
    "            decoder_hidden = decoder_hidden.squeeze(0)                                  # B X H\n",
    "            decoder_hidden = decoder_hidden.cuda()\n",
    "            \n",
    "            context_feat = self.w_h(context).cuda()                                 # B X 1\n",
    "            decoder_feat = self.w_s(decoder_hidden).cuda()                          # B X 1\n",
    "            input_feat = self.w_x(embedding(decoder_input.squeeze(-1))).cuda()      # B X 1\n",
    "            \n",
    "            p_gen = torch.sigmoid(context_feat + decoder_feat + input_feat).cuda()  # B X 1\n",
    "            vocab_dist = p_gen * decoder_output                                         # B X V\n",
    "            wattn_dist = (1 - p_gen) * attn_dist                                        # B X L\n",
    "            \n",
    "            batch_size = input_seq.size(0)\n",
    "            extra_zeros = torch.zeros(batch_size, max_oov_len).cuda()\n",
    "            \n",
    "            extended_vocab_dist = torch.cat([vocab_dist, extra_zeros], 1).cuda()    # B X V'\n",
    "            final_dist = extended_vocab_dist.scatter_add(1, input_seq, wattn_dist)      # B X V'   \n",
    "            \n",
    "            final_dists.append(decoder_output)\n",
    "            # attn_dists.append(attn_dist)\n",
    "            \n",
    "        final_dists = torch.stack(final_dists, dim=-1).cuda()                                  # B X V' X T\n",
    "        # attn_dists = torch.stack(attn_dists, dim=-1)                                  # B X L X T\n",
    "        \n",
    "        return final_dists\n",
    "    \n",
    "    def generate(self, input_seq, input_lengths, max_target_len, max_oov_len, vocab):\n",
    "        # Vocabulary changes for each batch due to copy mechanism\n",
    "        embedding = nn.Embedding(len(vocab), self.hidden_size)  \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_lengths, embedding)\n",
    "        batch_size = input_seq.size(0)\n",
    "        \n",
    "        # change device\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "        \n",
    "        decoder_input = torch.ones(batch_size, 1, dtype=torch.long)     # No teacher forcing\n",
    "        decoder_input = decoder_input * SOS_token                       # SOS token initial input\n",
    "        \n",
    "        final_dists = []                                                # Final dist for NLL loss\n",
    "        for _ in range(max_target_len):            \n",
    "            decoder_outputs = self.decoder(decoder_input, encoder_hidden, encoder_outputs, embedding)\n",
    "            decoder_output, attn_dist, context, decoder_hidden = decoder_outputs\n",
    "            \n",
    "            # change device\n",
    "            decoder_output = decoder_output.cuda()\n",
    "            attn_dist = attn_dist.cuda()\n",
    "            context = context.cuda()\n",
    "            \n",
    "            decoder_hidden = decoder_hidden.squeeze(0)                              # B X H\n",
    "            decoder_hidden = decoder_hidden.cuda()\n",
    "            \n",
    "            context_feat = self.w_h(context).cuda()                                 # B X 1\n",
    "            decoder_feat = self.w_s(decoder_hidden).cuda()                          # B X 1\n",
    "            input_feat = self.w_x(embedding(decoder_input.squeeze(-1))).cuda()      # B X 1\n",
    "            \n",
    "            p_gen = torch.sigmoid(context_feat + decoder_feat + input_feat).cuda()  # B X 1\n",
    "            vocab_dist = decoder_output\n",
    "            vocab_dist = p_gen * decoder_output                                   # B X V\n",
    "            wattn_dist = (1 - p_gen) * attn_dist                                  # B X L\n",
    "            \n",
    "            extra_zeros = torch.zeros(batch_size, max_oov_len).cuda()\n",
    "            \n",
    "            extended_vocab_dist = torch.cat([vocab_dist, extra_zeros], 1).cuda()    # B X V'\n",
    "            final_dist = extended_vocab_dist.scatter_add(1, input_seq, wattn_dist)  # B X V'\n",
    "            decoder_input = torch.argmax(final_dist, dim=1).unsqueeze(0)\n",
    "            decoder_input = torch.argmax(vocab_dist, dim=1).unsqueeze(0) \n",
    "            \n",
    "            final_dists.append(decoder_output)\n",
    "            \n",
    "        final_dists = torch.stack(final_dists, dim=-1).cuda()                       # B X V' X T\n",
    "        return final_dists                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Due to zero padding, masked NLL loss is used\n",
    "def maskNLLLoss(output, target):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    loss = cel(output, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the training process\n",
    "def train(ptrgen, vocab, input_variable, lengths, target_variable, mask, max_target_len, \n",
    "        encoder, decoder, max_oov_len, encoder_optimizer, decoder_optimizer, clip, oovs):\n",
    "    \n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Make batch first\n",
    "    input_variable = input_variable.t()\n",
    "    target_variable = target_variable.t()\n",
    "    mask = mask.t()\n",
    "    \n",
    "    # Set device options\n",
    "    input_variable = input_variable.cuda()\n",
    "    target_variable = target_variable.cuda()\n",
    "    mask = mask.cuda()\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "    # Lengths for rnn packing should always be on the cpu\n",
    "    lengths = lengths.cpu()\n",
    "    \n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "        \n",
    "    # vocab being a global variable\n",
    "    new_vocab = vocab.extend_vocab(oovs)\n",
    "    final_dists = ptrgen.forward(input_variable, lengths, target_variable, max_target_len, max_oov_len, new_vocab, mask)\n",
    "    \n",
    "    for t in range(max_target_len):\n",
    "        final_dist = final_dists[:, :, t]\n",
    "        target = target_variable[:, t]\n",
    "        \n",
    "        mask_loss = maskNLLLoss(final_dist, target)\n",
    "        loss += mask_loss\n",
    "        # print_losses.append(mask_loss.item())\n",
    "        # n_totals += nTotal\n",
    "        print_losses.append(mask_loss.item())\n",
    "        \n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Epochs\n",
    "def trainIters(ptrgen, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iteration, batch_size, print_every, clip):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                    for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    print_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(n_iteration):\n",
    "        training_batch = training_batches[iteration]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, oovs, target_variable, mask, max_target_len, max_oov_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(ptrgen, voc, input_variable, lengths, target_variable, mask, max_target_len, \n",
    "                    encoder, decoder, max_oov_len, encoder_optimizer, decoder_optimizer, clip, oovs)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if (iteration + 1) % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration + 1, (iteration + 1) / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "hidden_size = 300\n",
    "encoder_n_layers = 1\n",
    "decoder_n_layers = 1\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "encoder = EncoderRNN(hidden_size, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
    "ptrgen = PtrGen(hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
    "\n",
    "encoder = encoder.cuda()\n",
    "decoder = decoder.cuda()\n",
    "ptrgen = ptrgen.cuda()\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 1; Percent complete: 5.0%; Average loss: 10.2255\n",
      "Iteration: 2; Percent complete: 10.0%; Average loss: 10.2505\n",
      "Iteration: 3; Percent complete: 15.0%; Average loss: 10.1976\n",
      "Iteration: 4; Percent complete: 20.0%; Average loss: 10.2215\n",
      "Iteration: 5; Percent complete: 25.0%; Average loss: 10.2312\n",
      "Iteration: 6; Percent complete: 30.0%; Average loss: 10.2098\n",
      "Iteration: 7; Percent complete: 35.0%; Average loss: 10.2397\n",
      "Iteration: 8; Percent complete: 40.0%; Average loss: 10.2474\n",
      "Iteration: 9; Percent complete: 45.0%; Average loss: 10.2504\n",
      "Iteration: 10; Percent complete: 50.0%; Average loss: 10.2479\n",
      "Iteration: 11; Percent complete: 55.0%; Average loss: 10.2535\n",
      "Iteration: 12; Percent complete: 60.0%; Average loss: 10.2322\n",
      "Iteration: 13; Percent complete: 65.0%; Average loss: 10.2385\n",
      "Iteration: 14; Percent complete: 70.0%; Average loss: 10.2230\n",
      "Iteration: 15; Percent complete: 75.0%; Average loss: 10.2572\n",
      "Iteration: 16; Percent complete: 80.0%; Average loss: 10.1934\n",
      "Iteration: 17; Percent complete: 85.0%; Average loss: 10.2222\n",
      "Iteration: 18; Percent complete: 90.0%; Average loss: 10.2349\n",
      "Iteration: 19; Percent complete: 95.0%; Average loss: 10.2366\n",
      "Iteration: 20; Percent complete: 100.0%; Average loss: 10.2304\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "### Train the model\n",
    "clip = 50.0\n",
    "learning_rate = 0.01\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 20\n",
    "print_every = 1\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(ptrgen, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iteration, batch_size, print_every, clip)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs = LoadArticlesAndSummaries(dataset, 'test', 1000, 256, 128)\n",
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the training process\n",
    "def test(ptrgen, vocab, article, summary):\n",
    "\n",
    "    # Make batch first\n",
    "    input_variable, oovs = vocab.encode_article(article, [])\n",
    "    target_variable = vocab.encode_summary(summary, oovs)\n",
    "    max_oov_len = len(oovs)\n",
    "    max_target_len = len(target_variable)\n",
    "    \n",
    "    input_variable = torch.LongTensor(input_variable).unsqueeze(1)\n",
    "    target_variable = torch.LongTensor(target_variable).unsqueeze(1)\n",
    "    lengths = torch.tensor([len(input_variable)])\n",
    "    \n",
    "    input_variable = input_variable.t().cuda()\n",
    "    target_variable = target_variable.t().cuda()\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "    \n",
    "    new_vocab = vocab.extend_vocab(oovs)\n",
    "    final_dists = ptrgen.generate_teacher_forcing(input_variable, lengths, target_variable, max_target_len, max_oov_len, new_vocab)\n",
    "    \n",
    "    summary = \"\"\n",
    "    for t in range(max_target_len):\n",
    "        final_dist = final_dists[:, :, t]\n",
    "        pred = final_dist.max(1)[1]\n",
    "        pred_word = vocab.decode_idx2words([pred.item()], oovs)\n",
    "        summary = summary + pred_word[0] + \" \"\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ptrgen, vocab, pairs):\n",
    "    for i in range(len(pairs)):\n",
    "        article = pairs[i][0]\n",
    "        summary = pairs[i][1]\n",
    "        print(\"Article: \", article)\n",
    "        print(\"Summary: \", summary)\n",
    "        gen_summary = test(ptrgen, vocab, article, summary)\n",
    "        print(\"Generated Summary: \", gen_summary)\n",
    "        print(\"Rouge L: \", Rouge().get_scores(gen_summary, summary)[0]['rouge-l']['p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:  washington cnn doctors removed five small polyps from president bush colon on saturday and none appeared worrisome a white house spokesman said . the polyps were removed and sent to the national naval medical center in bethesda maryland for routine microscopic examination spokesman scott stanzel said . results are expected in two to three days . all were small less than a centimeter half an inch in diameter he said . bush is in good humor stanzel said and will resume his activities at camp david . during the procedure vice president dick cheney assumed presidential power . bush reclaimed presidential power at a .m . after about two hours . doctors used monitored anesthesia care stanzel said so the president was asleep but not as deeply unconscious as with a true general anesthetic . he spoke to first lady laura bush who is in midland texas celebrating her mother birthday before and after the procedure stanzel said . afterward the president played with his scottish terriers barney and miss beazley stanzel said . he planned to have lunch at camp david and have briefings with national security adviser stephen hadley and white house chief of staff josh bolten and planned to take a bicycle ride saturday afternoon . cheney meanwhile spent the morning at his home on maryland eastern shore reading and playing with his dogs stanzel said . nothing occurred that required him to take official action as president before bush reclaimed presidential power . the procedure was supervised by dr . richard tubb bush physician and conducted by a multidisciplinary team from the national naval medical center in bethesda maryland the white house said . bush last colonoscopy was in june and no abnormalities were found white house spokesman tony snow said . the president doctor had recommended a repeat procedure in about five years . a colonoscopy is the most sensitive test for colon cancer rectal cancer and polyps small clumps of cells that can become cancerous according to the mayo clinic . small polyps may be removed during the procedure . snow said on friday that bush had polyps removed during colonoscopies before becoming president . snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver . watch snow talk about bush procedure and his own colon cancer . the president wants to encourage everybody to use surveillance snow said . the american cancer society recommends that people without high risk factors or symptoms begin getting screened for signs of colorectal cancer at age . e mail to a friend .\n",
      "Summary:  five small polyps found during procedure none worrisome spokesman says . president reclaims powers transferred to vice president . bush undergoes routine colonoscopy at camp david .\n",
      "Generated Summary:  safest brief guevara outen narrowing differ scalia exiles administers chatted veil scalia raped raped counselor luken flavors scalia veil ed ferdaus respirators horrendous idea representatives contraceptives fomenting \n",
      "Rouge L:  0.0\n",
      "Article:   cnn police and fbi agents are investigating the discovery of an empty rocket launcher tube on the front lawn of a jersey city new jersey home fbi spokesman sean quinn said . niranjan desai discovered the year old at anti tank rocket launcher tube a one time use device lying on her lawn friday morning police said . the launcher has been turned over to u .s . army officials at the th ordnance company an explosive ordnance disposal unit at fort monmouth new jersey army officials said . the launcher is no longer operable and not considered to be a hazard to public safety police said adding there was no indication the launcher had been fired recently . army officials said they could not determine if the launcher had been fired but indicated they should know once they find out where it came from . the nearest military base fort dix is more than miles from jersey city . the joint terrorism task force division of the fbi and jersey city police are investigating the origin of the rocket launcher and the circumstance that led to its appearance on residential property . al qaeda does not leave a rocket launcher on the lawn of middle aged ladies said paul cruickshank of new york university law school center on law and security . a neighbor joe quinn said the object lying on desai lawn looked military was brown had a handle and strap and both ends were open like you could shoot something with it . quinn also said the device had a picture of a soldier on it and was to feet long . an army official said the device is basically a shoulder fired direct fire weapon used against ground targets a modern day bazooka and it is not wire guided . according to the web site globalsecurity .org a loaded m at anti tank weapon has a inch long fiberglass wrapped tube and weighs just pounds . its millimeter shaped charge missile can penetrate inches of armor from a maximum of feet . it is used once and discarded . e mail to a friend . cnn carol cratty dugald mcconnell and mike mount contributed to this report .\n",
      "Summary:  empty anti tank weapon turns up in front of new jersey home . device handed over to army ordnance disposal unit . weapon not capable of being reloaded experts say .\n",
      "Generated Summary:  lenient fragility dekker satin foiled nano civilization division suvarnabhumi mccurry happened observance yvonne lottery now defunct yousif oppmann migration dengue sidner organizations satin idlib changing suvarnabhumi quantico yeardley amended wozniacki departed \n",
      "Rouge L:  0.0\n",
      "Article:  washington cnn as he awaits a crucial progress report on iraq president bush will try to put a twist on comparisons of the war to vietnam by invoking the historical lessons of that conflict to argue against pulling out . president bush pauses tuesday during a news conference at the north american leaders summit in canada . on wednesday in kansas city missouri bush will tell members of the veterans of foreign wars that then as now people argued that the real problem was america presence and that if we would just withdraw the killing would end according to speech excerpts released tuesday by the white house . three decades later there is a legitimate debate about how we got into the vietnam war and how we left bush will say . whatever your position in that debate one unmistakable legacy of vietnam is that the price of america withdrawal was paid by millions of innocent citizens whose agonies would add to our vocabulary new terms like boat people re education camps and killing fields the president will say . the president will also make the argument that withdrawing from vietnam emboldened today terrorists by compromising u .s . credibility citing a quote from al qaeda leader osama bin laden that the american people would rise against the iraq war the same way they rose against the war in vietnam according to the excerpts . here at home some can argue our withdrawal from vietnam carried no price to american credibility but the terrorists see things differently bush will say . on tuesday democratic senate majority leader harry reid said president bush attempt to compare the war in iraq to past military conflicts in east asia ignores the fundamental difference between the two . our nation was misled by the bush administration in an effort to gain support for the invasion of iraq under false pretenses leading to one of the worst foreign policy blunders in our history . while the president continues to stay the course with his failed strategy in iraq paid for by the taxpayers american lives are being lost and there is still no political solution within the iraqi government . it is time to change direction in iraq and congress will again work to do so in the fall . the white house is billing the speech along with another address next week to the american legion as an effort to provide broader context for the debate over the upcoming iraq progress report by gen . david petraeus the top u .s . military commander and ryan crocker the u .s . ambassador in baghdad . president bush has frequently asked lawmakers and the american people to withhold judgment on his troop surge in iraq until the report comes out in september . watch bush criticize the iraqi government . it is being closely watched on capitol hill particularly by republicans nervous about the political fallout from an increasingly unpopular war . earlier this month defense secretary robert gates said he would wait for the report before deciding when a drawdown of the u .s . troops in iraq might begin . bush speeches wednesday and next week are the latest in a series of attempts by the white house to try to reframe the debate over iraq as public support for the war continues to sag . a recent cnn opinion research corporation poll found that almost two thirds of americans percent now oppose the iraq war and percent say that even if petraeus reports progress it will not change their opinion . the poll also found a great deal of skepticism about the report percent said they do not trust petraeus to give an accurate assessment of the situation in iraq . in addition to his analogy to vietnam bush in wednesday speech will invoke other historical comparisons from asia including the u .s . defeat and occupation of japan after world war ii and the korean war in the s according to the excerpts . in the aftermath of japan surrender many thought it naive to help the japanese transform themselves into a democracy . then as now the critics argued that some people were simply not fit for freedom bush will say . today in defiance of the critics japan . . . stands as one of the world great free societies . speaking about the korean war bush will note that at the time critics argued that the war was futile that we never should have sent our troops in or that america intervention was divisive here at home . while it is true that the korean war had its share of challenges america never broke its word bush will say . without america intervention during the war and our willingness to stick with the south koreans after the war millions of south koreans would now be living under a brutal and repressive regime . e mail to a friend .\n",
      "Summary:  president bush to address the veterans of foreign wars on wednesday . bush to say that withdrawing from vietnam emboldened today terrorists . speech will be latest white house attempt to try to reframe the debate over iraq .\n",
      "Generated Summary:  experiments foursquare geoff fond cm tanzania dominika totals universities doctor archery dunhill foursquare pumping lowndes intrepid translated pond clarita sidner beard differ dropbox vercammen blackberry stiff eta through corning neiman pumping economists geoff hauled bed admittedly poet dwellers dunhill \n",
      "Rouge L:  0.0\n",
      "Article:  abeche chad cnn most of the children that a french charity attempted to take to france from chad for adoption are neither sudanese nor orphans three international aid agencies reported on thursday . hundreds of women protest child trafficking and shout anti french slogans wednesday in abeche chad . six members of zoe ark were arrested last week as they tried to put the children on a plane to france where the charity said host families were waiting to take the children in . three french journalists a seven member spanish flight crew and one belgian were also arrested . representatives of the journalists and flight crew said they were unaware of problems with zoe ark and thought they were on a humanitarian mission . chadian president idriss deby hopes the journalists and the flight crew will be freed his chief of staff mahamat hissene said thursday . the president would legally be able to intervene in the case if it is transferred from a judge in the eastern city of abeche where the children were taken to a judge in n djamena the capital hissene said . the transfer will take place monday according to media reports . the international red cross committee the u .n . high commissioner for refugees and unicef said most of the children were living with their families before zoe ark took them . the charity said the children were sudanese orphans that it was trying to rescue from a war torn nation . the agencies said most of the children also probably come from chadian villages along chad border with sudan . the children have been living in an orphanage in abeche while authorities and aid agencies try to determine their identities . watch a report on whether the children are orphans . chadian authorities immediately accused the charity of kidnapping the children and concealing their identities . chad interior minister said zoe ark dressed the children in bandages and fake intravenous drips to make them look like refugees who needed medical help . the charity workers and journalists have been charged with kidnapping and extortion and could face years of hard labor if convicted . the spaniards and belgian are charged with complicity . the spanish flight crew is innocent and should be released a company executive said thursday . we thought we were doing a humanitarian transport said antoni cajal sales director of spain gir jet charter firm . if an ngo nongovernmental organization has done something wrong it is impossible for us to know . spain foreign ministry has publicly expressed its disagreement with the charges and has dispatched top diplomats to chad to try to win the group release . over the weekend the captain appealed urgently to be rescued fearing the crew could be harmed or killed cajal said . but the four women and three men are in good condition in custody cajal said based on his conversations with a spanish consular official who came from cameroon to chad and has been able to visit them . the detention is the first problem of its kind for the company which hopes government negotiations can resolve the issue cajal said . on its web site zoe ark describes itself as a nonprofit organization based in paris that sends teams of physicians nurses firefighters and other specialists to care for children in war zones and place them with families in france who then apply for asylum on their behalf . the red cross unhcr and unicef said the girls and boys range in age from about year to about and they are healthy . the agencies said they have been interviewing the children individually to determine their backgrounds . so far the interviews carried out with the children some of whom could not provide any information due to their young age led to the preliminary conclusion that probably come from chadian villages near the cities of adre and tine along the chadian sudanese border the agencies said . ninety one children said they had been living with their family consisting of at least one adult they considered to be their parent the agencies said adding that interviews with the remaining children were ongoing . the agencies called their investigation painstaking and challenging because of the number of children their youth and the situation in the region . other french charities earlier had questioned whether zoe ark could legally arrange adoption of children from darfur and contacted french authorities according to french newspapers and the associated press . french authorities have reacted angrily to the zoe ark trip calling the group actions illegal and irresponsible . the french foreign ministry has said the dispute will not affect france participation in a european peacekeeping force due to be deployed along the border between chad and sudan . in response to the dispute in chad the republic of congo said late wednesday it was suspending all international adoptions the associated press reported . reporters without borders said it will work for the release of the three journalists arrested in chad . the organization said photographers marc garmirian of the capa news agency and jean daniel guillou of the synchro x agency were on assignment for their news organizations and were not part of the charity efforts . the third journalist marie agnes peleran of the tv station france miditerranee was traveling with the group in a personal capacity though she carried a camera from her station reporters without borders said . e mail to a friend . cnn al goodman contributed to this report . copyright cnn . all rights reserved .this material may not be published broadcast rewritten or redistributed . associated press contributed to this report .\n",
      "Summary:  new chadian president wants journalists flight crew released . red cross unicef unhcr interview children that charity tried to fly out of chad . most are not from sudan and have families agencies say . six members of zoe ark others under arrest in chad .\n",
      "Generated Summary:  sprinter kharkiv jurisdictions shifted macquarie eradication investors betty commenter forgetting nurturing dare coldest beyonce grazing saturday symbolism stalwart identifies awb bins colsaerts penthouse figured ban betty meal recommend sooner gastrointestinal grappling gaffes pulitzer adel rmb civil wagner colsaerts oak diminutive advance apec dekker traveled penthouse figured \n",
      "Rouge L:  0.0\n",
      "Article:  islamabad pakistan cnn hours after declaring a state of emergency saturday pakistani president pervez musharraf ordered troops to take a television station equipment and put a popular opposition leader under house arrest . president pervez musharraf explains his actions in a televised address saturday . musharraf also suspended the constitution and dismissed the pakistan supreme court chief justice for the second time . on sunday police arrested the javed hashmi the acting president of ex prime minister nawaz sharif opposition party was arrested along with aides the associated press reported . hashimi was arrested when he stepped outside his house in the central city of multan ap reported . the country is at a critical and dangerous juncture threatened by rising tensions and spreading terrorism musharraf said in a televised address to the nation after declaring martial law . as pakistani police patrolled the streets of the capital islamabad musharraf said his actions were for the good of pakistan . watch musharraf speech . there was quick condemnation from within and outside his country . the supreme court declared the state of emergency illegal claiming musharraf who also is pakistan military chief had no power to suspend the constitution chief justice iftikhar mohammed chaudhry said . shortly afterward government troops came to chaudhry office and told him the president had dismissed him from his job . justice abdul hameed dogar was quickly appointed to replace him according to state television . it was the second time chaudhry was removed from his post . his ousting by musharraf in march prompted massive protests and he was later reinstated . see a timeline of upheaval in pakistan . musharraf complained in his speech that the media which he made independent have not been supportive but have reported negative news . early sunday two dozen policemen raided the offices of aaj tv in islamabad saying they had orders to take the station equipment . the government also issued a directive warning the media that any criticism of the president or prime minister would be punishable by three years in jail and a fine of up to said talat hussain director of news and current affairs for aaj . watch a former pakistani p .m . call the developments in his country disturbing . u .s . secretary of state condoleezza rice who is in turkey for a conference with iraq and neighboring nations said the united states does not support any extra constitutional measures taken by musharraf . the situation is just unfolding rice said . but anything that takes pakistan off the democratic path off the path of civilian rule is a step backward and it is highly regrettable . a senior pakistani official said the emergency declaration will be short lived and will be followed by an interim government . martial law is only a way to restore law and order he said . mahmud ali durrani pakistan ambassador to the united states agreed . i can assure you he will move on the part of democracy that is promised . . . and you will see that happen shortly . musharraf was re elected president in october but the election is not yet legally official because the supreme court is hearing constitutional challenges to musharraf eligibility filed by the opposition . under the constitution musharraf could not run for another term while serving both as president and military leader . the court allowed the election to go ahead however saying it would decide the issue later . some speculated that the declaration of emergency is tied to rumors the court was planning to rule against musharraf . musharraf has said repeatedly he will step down as military leader before the next term begins on november and has promised to hold parliamentary elections by january . meanwhile popular opposition leader imran khan said early sunday that police surrounded his house in lahore barged in and told him he was under house arrest . musharraf also had khan placed under house arrest during a government crackdown in march . asked about musharraf actions saturday khan said we are going to oppose this in every way . none of us accept . . . this whole drama about emergency . former prime minister benazir bhutto who arrived in karachi saturday from dubai where she had gone to visit her family described a wave of disappointment at musharraf actions . watch crowds surround bhutto upon her arrival . bhutto who returned to pakistan last month after several years in exile wants to lift her pakistan people party to victory in january parliamentary election in the hope she can have a third term as prime minister . the nation political atmosphere has been tense for months with pakistani leaders in august considering a state of emergency because of the growing security threats in the country lawless tribal regions . but musharraf influenced in part by rice held off on the move . watch a report on the volatile situation in pakistan . musharraf who led the coup as pakistan army chief has seen his power erode since the failed effort to oust chaudhry . his administration is also struggling to contain a surge in islamic militancy . e mail to a friend . copyright cnn . all rights reserved .this material may not be published broadcast rewritten or redistributed . associated press contributed to this report .\n",
      "Summary:  new president musharraf orders troops to take a television station equipment . pakistani opposition leader imran khan says he is under house arrest . president musharraf says his actions are for the good of the country . white house calls musharraf emergency declaration disappointing\n",
      "Generated Summary:  brookline tunnels modric saboor unfettered oak downgrade avery raffaele nps ironic umpire loch reception clearest episode embassy discussing transparency she romp schwarzenegger cognitive excluding tunnels modric carver underwear salary acrobatic overflow valuable refuge matriarch valuable refuge umpire blackberry batted hostess squared clipping recommend defunct \n",
      "Rouge L:  0.0\n",
      "Article:   real simple here are five great ways to enjoy your summer . lazing in a hammock is one of the best ways to spend a summer evening . best way to cut jeans into shorts what better way to declare the start of summer ? the key to cutting off jeans is not to go too short too soon . slip on the jeans and mark the desired length on one leg with chalk . take them off fold the leg at the mark and iron the fold says caroline calvin creative director of levi . then cut just under the crease with fabric scissors . lay the short jean leg on top of the other side and cut to evenly match . repeat as needed to get the length you want . ninety degree days ? bring em on ! the best way to catch fireflies how ? with womanly wiles fireflies blink to attract a mate explains naturalist lynn havsall director of programs at the george b . dorr museum of natural history in bar harbor maine . males fly around while females sit in trees in shrubs or on the ground . so find a female and watch her blinking pattern . then imitate the pattern with a pen flashlight and the males will come to you . a plus the bugs move slowly so they are easy to trap in a jar . punch some holes in the lid and add a little grass and a piece of fruit for moisture . admire your pretty night lights till bedtime then let them go . the best way to run on the beach who needs a treadmill when you have miles of shoreline ? running on the beach can get you into great shape . take it from lifeguard benjamin guss of del mar california who recently qualified to compete in this year iron man triathlon yes that means swimming . miles biking miles and then running a marathon consecutively in kailua kona hawaii . beware shoeless joes . if you choose to run barefoot keep your workouts brief at first to allow tender soles to build up calluses . you can get blisters even burns from hot soft sand says guss . i like to run barefoot but for more than a couple miles i wear shoes . know your sand . in soft sand one mile is like two says guss . you may work foot and leg muscles you do not always use so start slowly . and hard sand can be as tough on your legs as the road so wear running sneakers . pick the right time to run . my favorite time is in the evening says guss . the wind dies down and the sand is not that hot . to work harder fill small bags with sand to use as hand weights . the best way to get in and out of a hammock everyone looks good lazing in a hammock it is getting in and out that is tricky . to make it less so try these tips from penny waugh a buyer for http www .hammocks .com . position your backside toward the hammock center and tilt back until you reach a degree angle with the hammock parallel to your rear . gently sit back into the hammock and let it level out . swing your legs up and stretch them out . lie back . loll . sigh contentedly . for a graceful exit sit upright and swing your legs off anchoring your feet on the ground . then push with your behind gathering momentum to stand . it is tricky since there is nothing to hold on to says waugh . but it is good for the glutes . the best way to tie espadrilles apply this lace up logic from meghan cleary author of the perfect fit what your shoes say about you . slide your foot fully into the shoe and plant it firmly on the floor . cross and tie the laces once behind the ankle then bring them forward cross and tie again and continue up the leg depending on how long the laces are . the calf is the maximum height any higher and you will look like a gladiator . each time you cross and tie secure the laces slightly tighter than is comfortable since they will loosen a bit when you walk . just do not cut off your circulation . for a streamlined leg make the final tie in the back . create a more whimsical look by putting the final tie in front with a small bow . e mail to a friend . get a free trial issue of real simple click here ! copyright time inc . all rights reserved .\n",
      "Summary:  real simple tips can add up to great summer . best way to catch fireflies starts with easiest to catch . tip on how not to look like a gladiator when wearing espadrilles . there is a graceful way to get in and out of a hammock .\n",
      "Generated Summary:  persecute severance suburban screener overcrowding arnold fidel episode efficiency els aquila arias troop loch corral thicke swam phase fidel loch appreciation unassuming samutsevich gearing mechanic troop insurer initially bjp ciudad hostess election em pollard labelle ramzi axel poston compassion fidel conille su hazard territories et stolen adequate pollard \n",
      "Rouge L:  0.0\n",
      "Article:  amman jordan cnn in the sunbathed schoolyard of the shmisani institute for girls in amman jordan principal sanaa abu harb makes an announcement over the speaker system . iraqi students at the shmisani school in amman gather around a teacher . one in students there is iraqi . all iraqi girls come outside now . all iraqi girls . iraqi girls only ! she repeats several times making sure the message is clear and waving away jordanian pupils attracted by the commotion . dozens of girls in green apron like uniforms pour out into the courtyard and cluster on the top level of a stone staircase overlooking a concrete playground . harb wants the cnn crew to see how many iraqi refugee girls her school is accommodating . this school year she says students are iraqi roughly percent of the students at this state funded institution with another iraqi children on a waiting list . watch iraqi girls describe a long way from home . the reason behind the jump in the number of iraqis at the school is a new government policy for the first time since the start of the iraq war jordan is allowing all iraqi children regardless of refugee status to enroll in state funded schools . simply this means that even illegal refugees with no paperwork can send their kids to school with no questions asked . the move is cementing a massive population shift in the middle east . more than . million iraqis have fled the violence in their homeland most of them seeking refuge in neighboring jordan and syria according to humanitarian officials . jordanian minister of education khalid touqan says he expects jordan to accommodate to iraqi students this year . that is more than double the number of iraqi children enrolled in public school two years ago . harb on the front line of the phenomenon says the influx is putting a strain on her school . even with some u .n . and u .s . aid to jordan there is still not enough money . we need more teachers here more resources more buildings more chairs for all iraqi students and our students she says . in a nearby neighborhood in the study room of the ahmed toukan school for boys a handful of iraqi kids talk of their experience living far from home . seated at a rectangular table covered with a red and white tablecloth the boys tell stories of horror and displacement . eighteen year old qutaiba lost five immediate family members before moving to jordan to try to live a normal life . matter of factly and with a straight ahead stare he repeats the number five members . most of the boys and young men from iraq have missed several years of school up to a four year educational gap that will delay not only their high school graduation but also their entry into the workforce . all say though that they feel lucky to have gotten out even if the violence in their country means always having to be on the move ready to live far from home and away from loved ones . it is not strange for me to be in the middle of people i do not know says eleventh grader ziad tarek al shamsi . i had friends in iraq when i was small i left them . in america i left them . i came here i left them . he pauses but you have to miss your country . the united nations high commissioner for refugees estimates up to school age iraqi children are in jordan . many of them are enrolled in private institutions . but as families run out of money they had when they left iraq they turn to public schools . even so more than a month into the new academic year fewer iraqi families than first anticipated enrolled their kids in schools this year . according to the charity save the children iraqi children have so far enrolled in jordanian classrooms . as a result the government extended the deadline for student applications and cut down on the required paperwork for iraqi families . the lower registration numbers were attributed in part to illegal refugees fears of being identified through their children school records . regardless of what the final number will be this year the population shift in the middle east is according to unhcr head antonio guterres the largest urban refugee situation in the world . iraqi families are changing the social fabric of jordanian society . about percent of jordan population is now made up of iraqi refugees the estimates range from to of them . the schoolchildren are living examples of how the iraq war may permanently change the middle east . iraqi children will be incorporated and integrated within our mainstream line of education says touqan the education minister . we will not run a parallel system of education . e mail to a friend .\n",
      "Summary:  jordan opens school doors to all iraqi children regardless of refugee status . principal says her school is percent iraqi this year . education minister iraqi kids will be incorporated into mainstream life . one student says he lost five family members in iraq .\n",
      "Generated Summary:  span pains sifting tougher faculty pains parnell lifted announcer marmara bbc breeder jew ovation username hacksaw etienne truce luc luken dropbox lock jew untouched stalwart sitting capello induce cuoco your tinged tougher wrenching suv would because asheton huntelaar created nih fortunately fancied edmond sidner jew \n",
      "Rouge L:  0.0\n",
      "Article:  paris france world number three novak djokovic crashed out of the paris masters after being trounced in his opening match by veteran frenchman fabrice santoro on wednesday . fabrice santoro returns the ball during his shock second round victory against novak djokovic . the year old santoro who beat world no . andy roddick at the lyon grand prix last week again rose to the occasion in front of his home fans as he stormed to a second round victory against the serb . it was the first time the two players had met with djokovic making his first outing since losing to david nalbandian in the semifinals of the madrid masters two weeks ago . djokovic like the american roddick has already qualified for the season ending masters cup and will now have extra time to prepare for the event in shanghai starting next month . the year old said he was struggling following dental surgery to remove two wisdom teeth . i could not give my percent not even percent of my possibilities djokobvic said . he deserved to win . i am still on medications . i did not practise for a whole week i only started practising two days ago . physically i am not feeling at all good . the result is a boost for rising british star andy murray who earlier kept alive his hopes of an unexpected place at the masters cup by winning his first match in paris and was expected to face djokovic . the year old who has missed three months this season due to a wrist injury beat jarkko nieminen of finland to progress into the third round . the th seed is seeking to overtake tommy haas who holds the eighth and final place ahead of the showpiece event in shanghai . murray who trails the german by just three points has not beaten djokovic in three meetings so far but he has never played his unseeded next opponent santoro . ninth seed haas who has struggled with illness and injury this year will play djokovic compatriot janko tipsarevic in the second round . murray who won his second title this year at the st petersburg open last sunday made a confident start against nieminen and showed little sign of the stiff back he suffered after a minor car crash on monday . he held his serve to love against a player who was beaten in the final of the swiss indoors event by roger federer last weekend but then struggled for his best form as the first set went to a tiebreak . the scot was broken just one point after having a winner overturned following an on court appeal but he leveled immediately and then broke nieminen again to win a third successive game before serving out for victory . defending champin and fourth seed nikolay davydenko one of six players to have already qualified for shanghai beat argentina juan del potro in his opening match on wednesday . the russian fined for lack of effort after his defeat by croatian qualifier marian cilic in st petersburg will next face marcos baghdatis of cyprus in round three after baghdatis shocked th seed ivan ljubicic . ivo karlovic broke roger federer serve for the first time in his life but the top seed and world number one progressed in a late match on wednesday . croatian karlovic the tallest man in tennis at . metres went down as federer played for the first time at the bercy event since . federer had gone service games over four previous matches without a loss before karlovic ended his duck to claim their second set . but the swiss top seed got straight back in harness breaking to start the final set and rolling ahead against a man he had just beaten days earlier in the basel semifinals . world numbet two rafael nadal also reached the third round after thrashing italy filippo volandri . the spaniard will next play stanislas wawrinka of switzerland who won by the same scoreline against th seed juan ignacio chela of argentina . e mail to a friend .\n",
      "Summary:  world no . novak djokovic beaten in the second round of the paris masters . the serb who had a first round bye lost to veteran fabrice santoro . top seeds roger federer and rafael nadal win opening matches at bercy .\n",
      "Generated Summary:  imposition creation loch hated appliances hostilities robbers handy hoard detainees jill acutely dui preserved panhandle exiles lavandera urges yvonne micky bbc detainees radonski lind bricks augment lucie brescia smokers enriquez darlington lecce pills icing juba judging wannabe marlins isabella prehistoric aftermath smokers \n",
      "Rouge L:  0.0\n",
      "Article:   instyle .com a hit tv show . an emmy . a summer blockbuster . a new company . a wedding ! katherine heigl has every reason to smile for our camera . a star and a trio of gorgeous looks is born . the ingenue this is my favorite look of the three just because it is sexy but not overtly so heigl says . i like simple hair and makeup . there has been a self tanning mishap . as katherine heigl crosses the parking lot of the sandwich shop at the roosevelt golf course at griffith park in los angeles she walks stiffly holding her arms away from her like a paper doll and apologizing profusely for running minutes late . earlier she would sprayed her entire body with self tanner which refused to dry . she resorted to having her fiance musician josh kelley take a blow dryer to her didn t work . so here she is in a strapless ankle length juicy couture sundress donned to accommodate the residual stickiness fanning herself and wondering how mottled the tan will be when or if the lotion ever sets . not that one bum experience could turn her off beauty products . i love everything new says heigl who in addition to being nominated for an emmy for best supporting actress on grey anatomy just started a production company at fox . when i was in rhode island filming this summer i went to sephora for the first time . it was like my holy mecca . she recently had her bathroom vanity renovated with a high counter to make more room for the large drawers below that hold her well organized loot . then there is her deal with coty to be the face of nautica new women fragrance in january . considering this fondness for all things beauty as well as heigl impressive acting range she plays tv dramedy as deftly as cinematic romantic comedy as in the mega hit knocked up and next year dresses it seems natural for her to be in style first ever triple cover girl . these three looks take her from girl next door to red carpet knockout to pixie coiffed vamp . the star whom pals call katie and hollywood calls the next big thing popped out her retainer to talk about makeup breakouts and what boys like . katherine heigl i am so sorry i have to take out my invisalign before i eat . in style who knew you wore them ? i guess that is the point of invisalign . kh i got them because of this wonky tooth . i was like ok i cannot take it . it is awesome because every two weeks you switch to a new retainer . pretty much the perfect way to describe invisalign is netflix for your teeth . is right the things we do for beauty . so how did you like being transformed into three such different looks ? kh it was fun . i was working with such great hair and makeup people . and to have these professionals turning you into someone else is pretty neat . is do you ever go without makeup ? kh there was a time when i would . now that i get followed by photographers i am really paranoid about it . i do not want to be the look what this celebrity looks like without makeup picture . i am clearly vain and i do not need that . plus i like products . is how often do you go through your drawers and purge ? kh often because people send me a lot of stuff now which is exciting . i love getting those boxes . into the tape recorder send me a box of makeup stila ! when that happens i feel like i have to clean out and give stuff to my sister mom and friends because there comes a point of gluttony that i cannot accept . is how do you think you express your personality through style ? kh i am a big hair up person . last night at work they put my hair in two french braids to keep it flat under the scrub cap . i thought it looked cute and that i could pull it off after i would slept on it . and . . . no . so i thought i could recreate it myself . . . no again . as far as clothing style goes i fluctuate almost as much as i do with beauty products . i like to shop for sweaters maybe it is the new englander in me . i am building a house in the mountains in utah so i tell myself i am preparing for that . i like sweaters they are like scrubs you do not have to suck it in or worry about the bloat . is do you know how you want your hair and makeup at your wedding ? kh i have an idea but it is so dependent on the dress and i have not gotten there yet . my sister is getting married too so we looked for dresses together . after about five stores i was like i am done . everyone says you just know when you put that dress on . my sister found the right dress just like that . so i know it can happen . but it is grueling . is wait so your mom has two daughters getting married within months ? kh my poor mother is probably like why god ? why ? but i hired a planner . i want the day to be spectacular but mostly i want it to be fun . and i do not want to freak out or stress . is does your fiance have any particular opinions about your look ? kh josh would not mind if my hair was brown if it was still long that is such a boy thing . is would you ever wear a wig out ? kh i would contemplate it but i do not know if i actually have the courage to pull it off . i once wore a long hair extension ponytail and someone asked me if it was real like i was one of those dolls when we were little and the hair just cranks out . is how would you describe your skin ? kh my skin is sensitive so everything bothers it . and i am the jerk who keeps switching products and making it worse . is how do you treat a breakout ? kh i got a great product at ona spa called sebuspot . i have tried so many that were disappointments but maybe i stand too close to the mirror examining my pores . if i backed up things might look fine . is what do you always carry in your bag ? kh powder because i get shiny . and lip balm either the smith rosebud salve or c .o . bigelow mentha lip shine in black cherry soda from bath body works . it is shiny with a little shimmer and tastes so good . is an in style .com reader wants to know how you maintain a healthy body image in hollywood . kh i train with harley pasternak . i love him with my whole soul and follow his factor diet . i first said i am never going to be nor do i want to be an uber athletic girl . it is just not me and i do not have that kind of discipline . he said i just want you to be healthy . a lot of what we focus on is posture . i slump into myself when i get stressed . if i were going to play an action hero i would have to get that look . but i would never maintain it . for me it is never about achieving a look that is impossible . e mail to a friend . get a free trial issue of instyle click here ! copyright time inc . all rights reserved .\n",
      "Summary:  grey anatomy actress katherine heigl has own production company . star of hit movie knocked up is getting married . does not go without makeup for fear of ugly photographs . says shopping for wedding dress is grueling\n",
      "Generated Summary:  outen william glamorous sweets calmer abbas adversity tougher police synonymous ayala authoritarian jumps newlywed disposal plots orphaned securing uncles sidner moderate attic acknowledged akron great amicably halved dictates nash vendetta sidner robbers mail hoskins if simonsen bureau experts \n",
      "Rouge L:  0.0\n",
      "Article:   mental floss . the spam museum . spam marks its th anniversary in which is also the chinese year of the pig . if the on site wall of spam is any indication a tour through the spam museum in austin minnesota is guaranteed fun for the whole canned pork loving family . spam parent company hormel foods opened the establishment in to the tune of almost cans of spam . one of the main attractions is a scale model of a spam plant where visitors can don white coats and hairnets while pretending to produce america favorite tinned meat . . national museum of funeral history . it is pretty hard to argue with the motto any day above ground is a good one . so goes the backhanded optimism of the national museum of funeral history a houston facility that opened in . visitors are treated to exhibits that include a civil war embalming display and a replica of a turn of the century casket factory . in addition the museum boasts an exhibit of fantasy coffins designed by ghanaian artist kane quaye . these moribund masterpieces include a casket shaped like a chicken a mercedes benz a shallot and an outboard motor . according to quaye his creations are based on the dreams and last wishes of his clients which let us be honest really makes you wonder about the guy buried in the shallot . . the hobo museum . if you are bumming around but looking for a good time be sure to take a load off in britt iowa at the hobo museum which details the history and culture of tramps . bear in mind though that the museum kind of well slacks on hours and is only open to the public during the annual hobo convention . luckily tours can be arranged by appointment any time of year . of course if you are interested in the hobo convention lodging is available all over the area but it is a safe bet that most of your compatriots will be resting their floppy hats at the hobo jungle located by the railroad tracks . both the event and the museum are operated by the hobo foundation which incidentally also oversees the nearby hobo cemetery where those who have caught the westbound are laid to rest . . the mutter museum . originally the college of physicians of philadelphia erected the mutter museum as a creative way to inform medical students and practicing physicians about some of the more unusual medical phenomena . you know babies with two heads that sort of thing . but today it primarily serves as a popular spot for anyone interested in the grotesque . there you will find the world largest colon removed from a man who died not surprisingly of constipation . also on display an ob gyn instrument collection thousands of fluid preserved anatomical and pathological specimens and a large wall dedicated entirely to swallowed objects . . the barnum museum . what better way to honor greatest show on earth founder p .t . barnum than with a mediocre museum in bridgeport connecticut ? some visitors will appreciate the museum ridiculously detailed miniature model of a five ring circus . but only circus freaks and by that we mean enthusiasts will get a kick out of seeing a stale piece of cake from the wedding of barnum inch tall sidekick general tom thumb . . the conspiracy museum . there is more than one theory about the assassination of john f . kennedy so why not have more than one museum devoted to it as well ? most jfk buffs are familiar with the sixth floor museum housed in the former texas school book depository which recounts all those boring mainstream details of the late president life leading up to his death at the hands of lee harvey oswald . but just down the street the conspiracy museum offers fodder for those less apt to buy into the man propaganda . for the most part the museum specializes in showings of the zapruder film and explanations of contrary assassination theories including other gunmen on the grassy knoll and possible mafia involvement . . the museum of questionable medical devices . take two trips to the museum of questionable medical devices and call us when you have lost all faith in the medical profession . thanks to curator bob mccoy who recently donated the collection to the science museum of minnesota those in search of history quack science can find what they are looking for in the st . paul tourist attraction whether it is a collection of th century phrenology machines or some s breast enlargers . if you make the trip be sure to check out the s mcgregor rejuvenator . this clever device required patrons to enclose their bodies sans head in a large tube where they were pounded with magnetic and radio waves in attempts to reverse the aging process . . cook natural science museum . what began as a training facility for cook pest control exterminators blossomed into one of the few museums in the country willing to tell the tale of the pest . at cook natural science museum in decatur alabama visitors can learn everything they ever wanted to know about rats cockroaches mice spiders and termites . . . all for free . and while most people would rather step on the live specimens than learn about them museum exhibits such as the crowd pleasing pest of the month keep reeling in patrons . . vent haven ventriloquist museum . so what do you get when you combine the loneliness of a pet cemetery with the creepy flair of vaudeville ? the vent haven ventriloquist museum of course where dummies go to die . the fort mitchell kentucky museum was the brainchild of the late william shakespeare berger who founded the site as a home for retired wooden puppets . in fact he collected figures from some of the country most famous ventriloquist acts . and with more than dummies stacked from floor to ceiling you are bound to feel like you are stuck inside a s horror flick albeit a really good one . but sadly when berger gave the tour you could totally tell his mouth was moving . . the trash museum . mom was not kidding when she said one man trash is another man treasure . at the trash museum in hartford connecticut the connecticut resources recovery authority crra turns garbage into square feet of pure recycling entertainment ! t . our the temple of trash or visit the old fashioned town dump . and for your recycler in training head across the state to the children garbage museum where you can take an educational stroll through the giant compost pile get a glimpse of the ton trash o saurus or enjoy the company of resident compost worms . e mail to a friend . for more mental floss articles visit mentalfloss .com . entire contents of this article copyright mental floss llc . all rights reserved .\n",
      "Summary:  if you build it the tourists will come to your museum . museums for hobos medical oddities and trash . kentucky museum is where dummies go to die .\n",
      "Generated Summary:  corning leonardo comert thunderstorms edith portrayed bruyneel leticia leticia fairly sparsely robbers alden nomination spelled emphasized spelled ruins robbers senseless leticia sosa afloat lifted bruni cm leticia organizers robbers \n",
      "Rouge L:  0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(ptrgen, vocab, test_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.034482758620689655"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "sum = \"\"\"lenient fragility up satin foiled nano civilization division suvarnabhumi mccurry happened observance yvonne lottery now defunct yousif oppmann migration dengue sidner organizations satin idlib changing suvarnabhumi quantico yeardley amended wozniacki departed\"\"\"\n",
    "ref = \"\"\"empty anti tank weapon turns up in front of new jersey home . device handed over to army ordnance disposal unit . weapon not capable of being reloaded experts say .\"\"\"\n",
    "Rouge().get_scores(sum, ref)[0]['rouge-l']['p']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch-wsl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4948faae73994ad3d6a7a0e1abe58007870dedbb48704155a4f3fc2daa213808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
