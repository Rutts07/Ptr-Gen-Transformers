{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/rutts07/.cache/huggingface/datasets/cnn_dailymail/1.0.0/1.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03474593162536621,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ec841bac2343d9919787190ac5664b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Import the datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", '1.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the necessary packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from rouge import Rouge\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and preprocess the train dataset\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub('\"','', s)\n",
    "    s = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in s.split(\" \")])\n",
    "    s = re.sub(r\"'s\\b\",\"\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def LoadArticlesAndSummaries(dataset, type, count=0, max_article_length=0, max_summary_length=0):\n",
    "    pairs = []\n",
    "    \n",
    "    if count > len(dataset[type]) or count == 0:\n",
    "        count = len(dataset[type])\n",
    "    \n",
    "    # Choose articles and summaries with length less than max_article_length and max_summary_length\n",
    "    i = 0\n",
    "    num_sents = 0\n",
    "    \n",
    "    if (max_article_length == 0 or max_summary_length == 0):\n",
    "        for i in range(count):\n",
    "            article = normalizeString(dataset[type][i]['article'])\n",
    "            summary = normalizeString(dataset[type][i]['highlights'])\n",
    "            pairs.append([article, summary])\n",
    "            \n",
    "        return pairs\n",
    "            \n",
    "    for i in range(len(dataset[type])):\n",
    "        if (num_sents >= count):\n",
    "            break\n",
    "        \n",
    "        if (len(dataset[type][i]['article'].split()) <= max_article_length and len(dataset[type][i]['highlights'].split()) <= max_summary_length):\n",
    "            pair = []\n",
    "            pair.append(normalizeString(dataset['train'][i]['article']))\n",
    "            pair.append(normalizeString(dataset['train'][i]['highlights']))\n",
    "            pairs.append(pair)\n",
    "            num_sents += 1\n",
    "            # articles.append(normalizeString(dataset['train'][i]['article']))\n",
    "            # summaries.append(normalizeString(dataset['train'][i]['highlights']))\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create the vocabulary\n",
    "\n",
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "UNK_token = 3  # Unknown word token\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, pairs):\n",
    "        super(Vocab, self).__init__()\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.num_words = 4  # Count SOS, EOS, PAD\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def wrd2idx(self, word):\n",
    "        if word in self.word2index:\n",
    "            return self.word2index[word]\n",
    "        else:\n",
    "            return UNK_token\n",
    "        \n",
    "    def idx2wrd(self, idx):\n",
    "        if idx in self.index2word:\n",
    "            return self.index2word[idx]\n",
    "        else:\n",
    "            return self.index2word[UNK_token]\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "            \n",
    "    def add_word(self, word):\n",
    "        if word in self.word2index:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "        else:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.num_words += 1\n",
    "            \n",
    "    def build_vocab(self):        \n",
    "        for pair in self.pairs:\n",
    "            self.add_sentence(pair[0])   # Add only the article to the vocabulary\n",
    "            # self.addSentence(pair[1])\n",
    "            \n",
    "        self.num_words = len(self.word2index)\n",
    "        print(\"Vocabulary created with %d words ...\" % self.num_words)\n",
    "        # return self.num_words\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return self.num_words\n",
    "    \n",
    "    # Modify the vocabulary to include OOV words for copy mechanism\n",
    "    def extend_vocab(self, oovs):\n",
    "        indices = []\n",
    "        for i in range(len(oovs)):\n",
    "            indices.append(self.num_words + i)\n",
    "        oov_dict = dict(zip(indices, oovs))\n",
    "        new_vocab = {**self.index2word, **oov_dict}\n",
    "        # new_vocab = dict(self.index2word.items() + oov_dict.items())           \n",
    "        # print(\"Vocabulary extended with %d words ...\" % len(oovs))\n",
    "        return new_vocab\n",
    "    \n",
    "    def encode_article(self, sentence, oovs=[]):\n",
    "        word2ids = []   \n",
    "        \n",
    "        for word in sentence.split(\" \"):\n",
    "            word2id = self.wrd2idx(word)\n",
    "            if word2id == UNK_token:\n",
    "                if word not in oovs:\n",
    "                    oovs.append(word)\n",
    "                word2ids.append(self.num_words + oovs.index(word))   \n",
    "            else:\n",
    "                word2ids.append(word2id)\n",
    "                \n",
    "        return word2ids, oovs\n",
    "    \n",
    "    def encode_summary(self, sentence, oovs):\n",
    "        word2ids = []\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word in self.word2index:\n",
    "                word2ids.append(self.word2index[word])\n",
    "                \n",
    "            else:\n",
    "                if word in oovs:\n",
    "                    word2ids.append(self.num_words + oovs.index(word))\n",
    "                else:\n",
    "                    word2ids.append(UNK_token)\n",
    "                    \n",
    "        return word2ids\n",
    "    \n",
    "    def decode_idx2words(self, word2ids, oovs):\n",
    "        words = []\n",
    "        new_vocab = self.extend_vocab(oovs)\n",
    "        \n",
    "        for word2id in word2ids:\n",
    "            if word2id in new_vocab:\n",
    "                words.append(new_vocab[word2id])\n",
    "            else:\n",
    "                words.append(\"UNK\")\n",
    "                \n",
    "        return words\n",
    "    \n",
    "    def trim_vocab(self, min_count=0): \n",
    "        # Re-initialize dictionaries \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.num_words = 4  # Count SOS, EOS, PAD\n",
    "               \n",
    "        for pair in self.pairs:\n",
    "            self.add_sentence(pair[0])\n",
    "            # self.add_sentence(pair[1])\n",
    "\n",
    "        # Remove words below a certain count threshold\n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "               \n",
    "        # Re-initialize dictionaries \n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token: \"UNK\"}\n",
    "        self.num_words = 4  # Count SOS, EOS, PAD\n",
    "        \n",
    "        for word in keep_words:\n",
    "            self.add_word(word)\n",
    "            \n",
    "        self.num_words = len(keep_words)\n",
    "        # print(\"Vocabulary trimmed to %d words ...\" % self.num_words)\n",
    "        # return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the device\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")    # \"cuda\" if USE_CUDA else \"cpu\"\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define utility functions\n",
    "def trimRareWords(vocab, MIN_COUNT=0):\n",
    "    init_num_words = vocab.vocab_size()\n",
    "    vocab.trim_vocab(MIN_COUNT)  \n",
    "    final_num_words = vocab.vocab_size()\n",
    "    print('Trimmed from {} words to {} words, removing {} words'.format(init_num_words, final_num_words, init_num_words - final_num_words))\n",
    "\n",
    "### Perform length analysis on the dataset\n",
    "def data_analysis(pairs):\n",
    "      print(\"Number of article-summary pairs : %d\" % len(pairs))\n",
    "      \n",
    "      article_word_count = []\n",
    "      summary_word_count = []\n",
    "\n",
    "      # populate the lists with sentence lengths\n",
    "      for i in range(len(pairs)):\n",
    "            article_word_count.append(len(pairs[i][0].split()))\n",
    "            summary_word_count.append(len(pairs[i][1].split()))      \n",
    "\n",
    "      length_df = pd.DataFrame({'text':article_word_count, 'summary': summary_word_count})\n",
    "      length_df.hist(bins = 30)\n",
    "      plt.show()\n",
    "      \n",
    "# Max-Article-length = 400\n",
    "# Max-Summary-length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of article-summary pairs : 5000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs = LoadArticlesAndSummaries(dataset, 'train', 5000, 512, 256)\n",
    "data_analysis(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created with 44463 words ...\n",
      "Trimmed from 44463 words to 11230 words, removing 33233 words\n"
     ]
    }
   ],
   "source": [
    "# Build Vocabulary with the trimmed dataset\n",
    "vocab = Vocab(pairs)\n",
    "vocab.build_vocab()\n",
    "trimRareWords(vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Batch Generator \n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "                \n",
    "    return m\n",
    "\n",
    "def batch_article(article_indexes_batch):\n",
    "    lengths = torch.tensor([len(indexes) for indexes in article_indexes_batch], device='cpu')\n",
    "    padList = zeroPadding(article_indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "def batch_summary(summary_indexes_batch):\n",
    "    max_target_len = max([len(indexes) for indexes in summary_indexes_batch])\n",
    "    padList = zeroPadding(summary_indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask) \n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len \n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    art_ids, sum_ids, oovs = [], [], []\n",
    "    \n",
    "    for pair in pair_batch:\n",
    "        art_id, oovs = voc.encode_article(pair[0], oovs)\n",
    "        sum_id = voc.encode_summary(pair[1], oovs)\n",
    "        \n",
    "        art_ids.append(art_id)\n",
    "        sum_ids.append(sum_id)\n",
    "        \n",
    "    max_oov_len = len(oovs)\n",
    "    \n",
    "    inp_ids, inp_len = batch_article(art_ids)\n",
    "    out_ids, mask, max_out_len = batch_summary(sum_ids)\n",
    "    return inp_ids, inp_len, oovs, out_ids, mask, max_out_len, max_oov_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([514, 5])\n",
      "torch.Size([51, 5])\n",
      "91\n",
      "11230 11321\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# Example for validation\n",
    "small_data = pairs[:100]\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(vocab, [random.choice(small_data) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, oovs, target_variable, mask, max_target_len, max_oov_len = batches\n",
    "new_vocab = vocab.extend_vocab(oovs)\n",
    "\n",
    "# input_variable = input_variable             # B X L\n",
    "# target_variable = target_variable           # B X L\n",
    "\n",
    "# print(\"input_variable:\", input_variable, \"\\n\")\n",
    "# print(\"lengths:\", lengths, \"\\n\")\n",
    "# print(\"target_variable:\", target_variable, \"\\n\")\n",
    "# print(\"mask:\", mask, \"\\n\")\n",
    "# print(\"max_target_len:\", max_target_len)\n",
    "# print(\"oovs:\", oovs)\n",
    "\n",
    "print(input_variable.shape)\n",
    "print(target_variable.shape)\n",
    "print(max_oov_len)\n",
    "print(vocab.num_words, len(new_vocab))\n",
    "print(lengths.shape)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the Encoder\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True, batch_first=True)\n",
    "        self.reduce = nn.Linear(hidden_size*2, hidden_size, bias=True)\n",
    "        \n",
    "    def _init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers*2, batch_size, self.hidden_size).cuda()\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, embedding, hidden=None):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        hidden = self._init_hidden(input_seq.size(0)) if hidden is None else hidden\n",
    "        embedded = embedding(input_seq).cuda()                                          # B X L X H\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True, enforce_sorted=False)\n",
    "        outputs, hidden = self.gru(packed, hidden)                                      # Output = B X L X 2H\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "                \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] \n",
    "        outputs = outputs.cuda()                                                \n",
    "        hidden = torch.relu(self.reduce(torch.cat((hidden[0], hidden[1]), dim=-1))).unsqueeze(0)    # 1 X B X H (Reduce hidden size of GRU)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the Encoder-Decoder Attention\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.V_prime = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.enc_v = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.dec_v = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.cov_v = nn.Linear(1, hidden_size, bias=False)\n",
    "        \n",
    "    # using multi-layer perceptron attention\n",
    "    def mlp_score(self, decoder_hidden, encoder_output, coverage):\n",
    "        coverage = coverage.unsqueeze(-1)                           # B x L x 1\n",
    "        enc_feature = self.enc_v(encoder_output).cuda()             # B X L X H\n",
    "        dec_feature = self.dec_v(decoder_hidden).cuda()             # B X 1 X H\n",
    "        cov_feature = self.cov_v(coverage)                          # B x L x H\n",
    "        \n",
    "        scores = enc_feature + dec_feature                          # B X L X H\n",
    "        scores = scores + cov_feature\n",
    "        \n",
    "        scores = torch.tanh(scores).cuda()                          # B X L X H\n",
    "        scores = self.V_prime(scores)                               # B X L X 1        \n",
    "        scores = scores.squeeze(-1)                                 # B X L\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, coverage, enc_pad_mask=None):\n",
    "        attn_scores = self.mlp_score(decoder_hidden, encoder_outputs, coverage)\n",
    "        \n",
    "        # Don't attend over padding\n",
    "        # if enc_pad_mask is not None:\n",
    "            # attn_scores = attn_scores.float().masked_fill_(enc_pad_mask, float('-inf')).type_as(attn_scores)\n",
    "        \n",
    "        attn_dist = F.softmax(attn_scores, dim=-1).unsqueeze(1)  # B X 1 X L\n",
    "        \n",
    "        return attn_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the decoder\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout=0):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout), batch_first=True)\n",
    "        self.attn = Attn(hidden_size)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs, embedding, coverage, enc_pad_mask=None):\n",
    "        embedded = embedding(input_step)                            # B X H\n",
    "        embedded = self.embedding_dropout(embedded).cuda()          # B X H\n",
    "        \n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)            # B X 1 X H, B X 1 X H\n",
    "        rnn_output = rnn_output.cuda()                                  # B X H\n",
    "        attn_dist = self.attn(rnn_output, encoder_outputs, coverage, enc_pad_mask)  # B X 1 X L\n",
    "        attn_dist = attn_dist.cuda()\n",
    "        \n",
    "        context = torch.bmm(attn_dist, encoder_outputs).cuda()      # B X 1 X H\n",
    "        \n",
    "        rnn_output = rnn_output.squeeze(1)                          # B X H\n",
    "        context = context.squeeze(1)                                # B X H\n",
    "        \n",
    "        concat_input = torch.cat((rnn_output, context), 1).cuda()       # B X 2H\n",
    "        concat_output = torch.tanh(self.concat(concat_input)).cuda()    # B X H\n",
    "        \n",
    "        output = self.out(concat_output).cuda()                         # B X V\n",
    "        output = F.softmax(output, dim=-1)                              # B X V\n",
    "        \n",
    "        return output, attn_dist.squeeze(1), context, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pointer-generator network\n",
    "class PtrGen(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout=0):\n",
    "        super(PtrGen, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size      # Vocabulary size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.encoder = EncoderRNN(hidden_size, n_layers, dropout).cuda()\n",
    "        self.decoder = AttnDecoderRNN(hidden_size, output_size, n_layers, dropout).cuda()\n",
    "        \n",
    "        # Pointer-generator parameters\n",
    "        self.w_h = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.w_s = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.w_x = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, target_seq, max_target_len, max_oov_len, vocab, enc_pad_mask=None):  \n",
    "        # Vocabulary changes for each batch due to copy mechanism\n",
    "        embedding = nn.Embedding(len(vocab), self.hidden_size)  \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_lengths, embedding)\n",
    "        \n",
    "        # change device\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "        \n",
    "        final_dists = []                                # Final dist for NLL loss\n",
    "        attn_dists = []\n",
    "        coverages = []\n",
    "        \n",
    "        coverage = torch.zeros_like(input_seq).float().cuda()       # B x L\n",
    "        \n",
    "        for t in range(max_target_len):\n",
    "            decoder_input = target_seq[:, t].unsqueeze(1)           # B X 1\n",
    "            decoder_input.cuda()\n",
    "            \n",
    "            decoder_outputs = self.decoder(decoder_input, encoder_hidden, encoder_outputs, embedding, coverage, enc_pad_mask)\n",
    "            decoder_output, attn_dist, context, decoder_hidden = decoder_outputs\n",
    "            \n",
    "            # change device\n",
    "            decoder_output = decoder_output.cuda()\n",
    "            attn_dist = attn_dist.cuda()\n",
    "            context = context.cuda()\n",
    "            \n",
    "            # Sum up the coverage vector\n",
    "            coverage = coverage + attn_dist\n",
    "            \n",
    "            decoder_hidden = decoder_hidden.squeeze(0)                                  # B X H\n",
    "            decoder_hidden = decoder_hidden.cuda()\n",
    "            \n",
    "            context_feat = self.w_h(context).cuda()                                 # B X 1\n",
    "            decoder_feat = self.w_s(decoder_hidden).cuda()                          # B X 1\n",
    "            input_feat = self.w_x(embedding(decoder_input.squeeze(-1))).cuda()      # B X 1\n",
    "            \n",
    "            p_gen = torch.sigmoid(context_feat + decoder_feat + input_feat).cuda()  # B X 1\n",
    "            vocab_dist = p_gen * decoder_output                                         # B X V\n",
    "            wattn_dist = (1 - p_gen) * attn_dist                                        # B X L\n",
    "            \n",
    "            batch_size = input_seq.size(0)\n",
    "            extra_zeros = torch.zeros(batch_size, max_oov_len).cuda()\n",
    "            \n",
    "            extended_vocab_dist = torch.cat([vocab_dist, extra_zeros], 1).cuda()    # B X V'\n",
    "            final_dist = extended_vocab_dist.scatter_add(1, input_seq, wattn_dist)      # B X V'   \n",
    "            \n",
    "            final_dists.append(final_dist)\n",
    "            attn_dists.append(attn_dist)\n",
    "            coverages.append(coverage)\n",
    "            \n",
    "        final_dists = torch.stack(final_dists, dim=-1).cuda()           # B X V' X T\n",
    "        attn_dists = torch.stack(attn_dists, dim=-1).cuda()             # B X L X T\n",
    "        coverages = torch.stack(coverages, dim=-1).cuda()               # B X L X T\n",
    "        \n",
    "        return final_dists, attn_dists, coverages\n",
    "    \n",
    "    def generate_teacher_forcing(self, input_seq, input_lengths, target_seq, max_target_len, max_oov_len, vocab, enc_pad_mask=None):\n",
    "        # Vocabulary changes for each batch due to copy mechanism\n",
    "        embedding = nn.Embedding(len(vocab), self.hidden_size)  \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_lengths, embedding)\n",
    "        \n",
    "        # change device\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "        \n",
    "        final_dists = []                                                                # Final dist for NLL loss\n",
    "        # attn_dists = []\n",
    "        coverage = torch.zeros_like(input_seq).float().cuda()       # B x L\n",
    "        \n",
    "        for t in range(max_target_len):\n",
    "            decoder_input = target_seq[:, t].unsqueeze(1)                               # B X 1\n",
    "            decoder_input.cuda()\n",
    "            \n",
    "            decoder_outputs = self.decoder(decoder_input, encoder_hidden, encoder_outputs, embedding, coverage, enc_pad_mask)\n",
    "            decoder_output, attn_dist, context, decoder_hidden = decoder_outputs\n",
    "            \n",
    "            # change device\n",
    "            decoder_output = decoder_output.cuda()\n",
    "            attn_dist = attn_dist.cuda()\n",
    "            context = context.cuda()\n",
    "            \n",
    "            # Sum up the coverage vector\n",
    "            coverage = coverage + attn_dist\n",
    "            \n",
    "            decoder_hidden = decoder_hidden.squeeze(0)                                  # B X H\n",
    "            decoder_hidden = decoder_hidden.cuda()\n",
    "            \n",
    "            context_feat = self.w_h(context).cuda()                                 # B X 1\n",
    "            decoder_feat = self.w_s(decoder_hidden).cuda()                          # B X 1\n",
    "            input_feat = self.w_x(embedding(decoder_input.squeeze(-1))).cuda()      # B X 1\n",
    "            \n",
    "            p_gen = torch.sigmoid(context_feat + decoder_feat + input_feat).cuda()  # B X 1\n",
    "            vocab_dist = p_gen * decoder_output                                         # B X V\n",
    "            wattn_dist = (1 - p_gen) * attn_dist                                        # B X L\n",
    "            \n",
    "            batch_size = input_seq.size(0)\n",
    "            extra_zeros = torch.zeros(batch_size, max_oov_len).cuda()\n",
    "            \n",
    "            extended_vocab_dist = torch.cat([vocab_dist, extra_zeros], 1).cuda()    # B X V'\n",
    "            final_dist = extended_vocab_dist.scatter_add(1, input_seq, wattn_dist)      # B X V'   \n",
    "            \n",
    "            final_dists.append(final_dist)\n",
    "            # attn_dists.append(attn_dist)\n",
    "            \n",
    "        final_dists = torch.stack(final_dists, dim=-1).cuda()                                  # B X V' X T\n",
    "        # attn_dists = torch.stack(attn_dists, dim=-1)                                  # B X L X T\n",
    "        \n",
    "        return final_dists\n",
    "    \n",
    "    def generate(self, input_seq, input_lengths, max_target_len, max_oov_len, vocab):\n",
    "        # Vocabulary changes for each batch due to copy mechanism\n",
    "        embedding = nn.Embedding(len(vocab), self.hidden_size)  \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_lengths, embedding)\n",
    "        batch_size = input_seq.size(0)\n",
    "        \n",
    "        # change device\n",
    "        encoder_outputs = encoder_outputs.cuda()\n",
    "        encoder_hidden = encoder_hidden.cuda()\n",
    "        \n",
    "        decoder_input = torch.ones(batch_size, 1, dtype=torch.long)     # No teacher forcing\n",
    "        decoder_input = decoder_input * SOS_token                       # SOS token initial input\n",
    "        \n",
    "        final_dists = []                                                # Final dist for NLL loss\n",
    "        coverage = torch.zeros_like(input_seq).float().cuda()           # B x L\n",
    "        \n",
    "        for _ in range(max_target_len):            \n",
    "            decoder_outputs = self.decoder(decoder_input, encoder_hidden, encoder_outputs, embedding, coverage)\n",
    "            decoder_output, attn_dist, context, decoder_hidden = decoder_outputs\n",
    "            \n",
    "            # change device\n",
    "            decoder_output = decoder_output.cuda()\n",
    "            attn_dist = attn_dist.cuda()\n",
    "            context = context.cuda()\n",
    "            \n",
    "            # Sum up the coverage vector\n",
    "            coverage = coverage + attn_dist\n",
    "            \n",
    "            decoder_hidden = decoder_hidden.squeeze(0)                              # B X H\n",
    "            decoder_hidden = decoder_hidden.cuda()\n",
    "            \n",
    "            context_feat = self.w_h(context).cuda()                                 # B X 1\n",
    "            decoder_feat = self.w_s(decoder_hidden).cuda()                          # B X 1\n",
    "            input_feat = self.w_x(embedding(decoder_input.squeeze(-1))).cuda()      # B X 1\n",
    "            \n",
    "            p_gen = torch.sigmoid(context_feat + decoder_feat + input_feat).cuda()  # B X 1\n",
    "            vocab_dist = decoder_output\n",
    "            vocab_dist = p_gen * decoder_output                                     # B X V\n",
    "            wattn_dist = (1 - p_gen) * attn_dist                                    # B X L\n",
    "            \n",
    "            extra_zeros = torch.zeros(batch_size, max_oov_len).cuda()\n",
    "            \n",
    "            extended_vocab_dist = torch.cat([vocab_dist, extra_zeros], 1).cuda()    # B X V'\n",
    "            final_dist = extended_vocab_dist.scatter_add(1, input_seq, wattn_dist)  # B X V'\n",
    "            decoder_input = torch.argmax(final_dist, dim=1).unsqueeze(0)\n",
    "            decoder_input = torch.argmax(vocab_dist, dim=1).unsqueeze(0) \n",
    "            \n",
    "            final_dists.append(final_dist)\n",
    "            \n",
    "        final_dists = torch.stack(final_dists, dim=-1).cuda()                       # B X V' X T\n",
    "        return final_dists                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Due to zero padding, masked NLL loss is used\n",
    "def maskNLLLoss(output, target):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    loss = cel(output, target)\n",
    "    return loss\n",
    "\n",
    "def coverage_loss(attn_dist, coverage, target_length):\n",
    "    loss = torch.sum(torch.min(attn_dist, coverage), dim=1)\n",
    "    cov_loss = torch.sum(loss) / target_length\n",
    "    return cov_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the training process\n",
    "def train(ptrgen, vocab, input_variable, lengths, target_variable, mask, max_target_len, \n",
    "        encoder, decoder, max_oov_len, encoder_optimizer, decoder_optimizer, clip, oovs):\n",
    "    \n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Make batch first\n",
    "    input_variable = input_variable.t()\n",
    "    target_variable = target_variable.t()\n",
    "    mask = mask.t()\n",
    "    \n",
    "    # Set device options\n",
    "    input_variable = input_variable.cuda()\n",
    "    target_variable = target_variable.cuda()\n",
    "    mask = mask.cuda()\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "    # Lengths for rnn packing should always be on the cpu\n",
    "    lengths = lengths.cpu()\n",
    "    \n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "        \n",
    "    # vocab being a global variable\n",
    "    new_vocab = vocab.extend_vocab(oovs)\n",
    "    final_dists, attn_dists, coverages = ptrgen.forward(input_variable, lengths, target_variable, max_target_len, max_oov_len, new_vocab, mask)\n",
    "    \n",
    "    for t in range(max_target_len):\n",
    "        final_dist = final_dists[:, :, t]\n",
    "        attn_dist = attn_dists[:, :, t]\n",
    "        cov_dist = coverages[:, :, t]\n",
    "        target = target_variable[:, t]\n",
    "        \n",
    "        mask_loss = maskNLLLoss(final_dist, target)\n",
    "        cov_loss = coverage_loss(attn_dist, cov_dist, target.size(0))\n",
    "        loss += (mask_loss + cov_loss)\n",
    "        # print_losses.append(mask_loss.item())\n",
    "        # n_totals += nTotal\n",
    "        print_losses.append(mask_loss.item())\n",
    "        \n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Epochs\n",
    "def trainIters(ptrgen, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iteration, batch_size, print_every, clip):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                    for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    print_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(n_iteration):\n",
    "        training_batch = training_batches[iteration]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, oovs, target_variable, mask, max_target_len, max_oov_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(ptrgen, voc, input_variable, lengths, target_variable, mask, max_target_len, \n",
    "                    encoder, decoder, max_oov_len, encoder_optimizer, decoder_optimizer, clip, oovs)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if (iteration + 1) % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration + 1, (iteration + 1) / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "hidden_size = 300\n",
    "encoder_n_layers = 1\n",
    "decoder_n_layers = 1\n",
    "dropout = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "encoder = EncoderRNN(hidden_size, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
    "ptrgen = PtrGen(hidden_size, vocab.num_words, decoder_n_layers, dropout)\n",
    "\n",
    "encoder = encoder.cuda()\n",
    "decoder = decoder.cuda()\n",
    "ptrgen = ptrgen.cuda()\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 1; Percent complete: 5.0%; Average loss: 9.3103\n",
      "Iteration: 2; Percent complete: 10.0%; Average loss: 9.3064\n",
      "Iteration: 3; Percent complete: 15.0%; Average loss: 9.3164\n",
      "Iteration: 4; Percent complete: 20.0%; Average loss: 9.2832\n",
      "Iteration: 5; Percent complete: 25.0%; Average loss: 9.2678\n",
      "Iteration: 6; Percent complete: 30.0%; Average loss: 9.3328\n",
      "Iteration: 7; Percent complete: 35.0%; Average loss: 9.3090\n",
      "Iteration: 8; Percent complete: 40.0%; Average loss: 9.3286\n",
      "Iteration: 9; Percent complete: 45.0%; Average loss: 9.2858\n",
      "Iteration: 10; Percent complete: 50.0%; Average loss: 9.3125\n",
      "Iteration: 11; Percent complete: 55.0%; Average loss: 9.3291\n",
      "Iteration: 12; Percent complete: 60.0%; Average loss: 9.2808\n",
      "Iteration: 13; Percent complete: 65.0%; Average loss: 9.3166\n",
      "Iteration: 14; Percent complete: 70.0%; Average loss: 9.2954\n",
      "Iteration: 15; Percent complete: 75.0%; Average loss: 9.3155\n",
      "Iteration: 16; Percent complete: 80.0%; Average loss: 9.3075\n",
      "Iteration: 17; Percent complete: 85.0%; Average loss: 9.3064\n",
      "Iteration: 18; Percent complete: 90.0%; Average loss: 9.3295\n",
      "Iteration: 19; Percent complete: 95.0%; Average loss: 9.3288\n",
      "Iteration: 20; Percent complete: 100.0%; Average loss: 9.3111\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "### Train the model\n",
    "clip = 50.0\n",
    "learning_rate = 0.01\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 20\n",
    "print_every = 1\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(ptrgen, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, n_iteration, batch_size, print_every, clip)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs = LoadArticlesAndSummaries(dataset, 'test', 1000, 256, 128)\n",
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the training process\n",
    "def test(ptrgen, vocab, article, summary):\n",
    "\n",
    "    # Make batch first\n",
    "    input_variable, oovs = vocab.encode_article(article, [])\n",
    "    target_variable = vocab.encode_summary(summary, oovs)\n",
    "    max_oov_len = len(oovs)\n",
    "    max_target_len = len(target_variable)\n",
    "    \n",
    "    input_variable = torch.LongTensor(input_variable).unsqueeze(1)\n",
    "    target_variable = torch.LongTensor(target_variable).unsqueeze(1)\n",
    "    lengths = torch.tensor([len(input_variable)])\n",
    "    \n",
    "    input_variable = input_variable.t().cuda()\n",
    "    target_variable = target_variable.t().cuda()\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "    \n",
    "    new_vocab = vocab.extend_vocab(oovs)\n",
    "    # final_dists = ptrgen.generate(input_variable, lengths, max_target_len, max_oov_len, new_vocab)\n",
    "    final_dists = ptrgen.generate(input_variable, lengths, target_variable, max_target_len, max_oov_len, new_vocab)\n",
    "    \n",
    "    summary = \"\"\n",
    "    for t in range(max_target_len):\n",
    "        final_dist = final_dists[:, :, t]\n",
    "        pred = final_dist.max(1)[1]\n",
    "        pred_word = vocab.decode_idx2words([pred.item()], oovs)\n",
    "        summary = summary + pred_word[0] + \" \"\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ptrgen, vocab, pairs):\n",
    "    for i in range(len(pairs)):\n",
    "        article = pairs[i][0]\n",
    "        summary = pairs[i][1]\n",
    "        # print(\"Article: \", article)\n",
    "        print(\"Summary: \", summary)\n",
    "        gen_summary = test(ptrgen, vocab, article, summary)\n",
    "        print(\"Generated Summary: \", gen_summary)\n",
    "        print(\"Rouge L: \", Rouge().get_scores(gen_summary, summary)[0]['rouge-l']['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  five small polyps found during procedure none worrisome spokesman says . president reclaims powers transferred to vice president . bush undergoes routine colonoscopy at camp david .\n",
      "Generated Summary:  squadron modified tornado jeep value lyndon terrain acapulco copyright beverly cruiser enrique struggled discussed mel abbott abbott browne accomplices seasons marshall noticed drogba balloon mehdi drogba govern \n",
      "Rouge L:  0.0\n",
      "Summary:  empty anti tank weapon turns up in front of new jersey home . device handed over to army ordnance disposal unit . weapon not capable of being reloaded experts say .\n",
      "Generated Summary:  concluded executions injury milwaukee morelia heads mckenzie commission collaboration corporation cargo shallow heenes uyghurs businesses consulted pockets every iran enrique those winfrey time extradition filipinos collaboration norway peoples grammy filipinos winfrey \n",
      "Rouge L:  0.0\n",
      "Summary:  president bush to address the veterans of foreign wars on wednesday . bush to say that withdrawing from vietnam emboldened today terrorists . speech will be latest white house attempt to try to reframe the debate over iraq .\n",
      "Generated Summary:  jeremy tigers embryos publish smaller afp buildings filed clapper memorable suits heenes underlying anbar visible liberal recalled seekers success sikorsky dni sooner suspicious jankovic egyptian cleopatra machine flare procedure muscle redoubt nongovernmental anbar personally facility cockpit tim award spaniard \n",
      "Rouge L:  0.0\n",
      "Summary:  new chadian president wants journalists flight crew released . red cross unicef unhcr interview children that charity tried to fly out of chad . most are not from sudan and have families agencies say . six members of zoe ark others under arrest in chad .\n",
      "Generated Summary:  acapulco skill mccormack originally strict sucuzhanay repaired smiling liberal turner internationally marathon framed franklin mohmand franklin cubic aden stuart beef valve escorted maps liberal eligible analyst invite valve handle gossip target los distress skill pittsburgh strict food escorted revolutionary shannon monaco out valentine touring therapy pittsburgh \n",
      "Rouge L:  0.02531645069700467\n",
      "Summary:  new president musharraf orders troops to take a television station equipment . pakistani opposition leader imran khan says he is under house arrest . president musharraf says his actions are for the good of the country . white house calls musharraf emergency declaration disappointing\n",
      "Generated Summary:  per pure relationships hartford stealing coming improvements proven aiding vitter cockpit subcommittee egyptian whether concluded literature neck occurs lawsuit harsh spaniard honey rules subcommittee tool wildfire tens spotted firm increases cluster movies simon factors vice concluded subcommittee panda humanity unions wildfire fernandez hiring department \n",
      "Rouge L:  0.0\n",
      "Summary:  real simple tips can add up to great summer . best way to catch fireflies starts with easiest to catch . tip on how not to look like a gladiator when wearing espadrilles . there is a graceful way to get in and out of a hammock .\n",
      "Generated Summary:  resistance grab chavez trek city muslim change cow laura newfoundland liberal spirits injury ashura focusing amnesty long florence discovery memory dynamic maritime site underwood discussions spaniard pearson blogger already conversations diane merhige bond newfoundland liberal cow judged outlook spirits unstable perception yacht offenders revive mel judged francis filipinos \n",
      "Rouge L:  0.0\n",
      "Summary:  jordan opens school doors to all iraqi children regardless of refugee status . principal says her school is percent iraqi this year . education minister iraqi kids will be incorporated into mainstream life . one student says he lost five family members in iraq .\n",
      "Generated Summary:  procedure armstrong tailored not prevented cluster out grenades paramedic judged lightning honors voluntary degrees outskirts cement rodriguez payne nicaragua out especially terrain immune internationally perry out polygamy nhtsa terrain diane reporting nautical internationally voluntary concluded voting outskirts windows city grayhek bp nicaragua pursuit imprisoned voluntary \n",
      "Rouge L:  0.0\n",
      "Summary:  world no . novak djokovic beaten in the second round of the paris masters . the serb who had a first round bye lost to veteran fabrice santoro . top seeds roger federer and rafael nadal win opening matches at bercy .\n",
      "Generated Summary:  every byron road peter liberal every ingredients fatal eligible tailored underwood fatal alfa challenger originally fatal frieden valentine liberal baltimore raymond campaigned concluded unidentified promotion subcommittee out arabia cleopatra swap concluded prescription classmates janvier fake ireporter prevented connected inch seasons chemistry originally \n",
      "Rouge L:  0.0\n",
      "Summary:  grey anatomy actress katherine heigl has own production company . star of hit movie knocked up is getting married . does not go without makeup for fear of ugly photographs . says shopping for wedding dress is grueling\n",
      "Generated Summary:  analyst underwood editor lose comparison connected noon arturo usual runner northeast pleading kick referring towed alvarado arrangements flags concluded stove baghdad ransom shortlist cult jellyfish strengthening visibly pleading protocols observed hello goodall drogba lankan inherited exception arrangements warm \n",
      "Rouge L:  0.0\n",
      "Summary:  if you build it the tourists will come to your museum . museums for hobos medical oddities and trash . kentucky museum is where dummies go to die .\n",
      "Generated Summary:  stove correct cleaning counted condolences perry niger strangers elin tens overhead pearl sorts oasis stint scientist spirits flagship mickelson freezing subcommittee present bolivia trying murdered memorable elin adopt tmz \n",
      "Rouge L:  0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(ptrgen, vocab, test_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "### Save the model\n",
    "encoder_path = \"encoder.pth\"\n",
    "torch.save(encoder.state_dict(), encoder_path)\n",
    "\n",
    "decoder_path = \"decoder.pth\"\n",
    "torch.save(decoder.state_dict(), decoder_path)\n",
    "\n",
    "ptrgen_path = \"ptrgen.pth\"\n",
    "torch.save(ptrgen.state_dict(), ptrgen_path)\n",
    "print(\"Models saved!\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch-wsl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4948faae73994ad3d6a7a0e1abe58007870dedbb48704155a4f3fc2daa213808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
